{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Tracker\n",
    "made by eyeol\n",
    "\n",
    "띄어쓰기 전처리 유무에 따른 tokenizing 결과를 비교해보고 싶어서 만들었음\n",
    "\n",
    "**주의점**\n",
    "\n",
    "파일 실행은 root 폴더(train.py와 동일한 위치)에서 해야 합니다<br/>\n",
    "baseline_config.yaml에서 inference의 model path를 원하는 모델 경로로 바꿔야 합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 모델 및 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "#import wandb\n",
    "##############################\n",
    "from utils import data_pipeline\n",
    "from model.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_path', default='./data/raw/train.csv')\n",
    "parser.add_argument('--dev_path', default='./data/raw/dev.csv')\n",
    "parser.add_argument('--test_path', default='./data/raw/dev.csv')\n",
    "##\n",
    "parser.add_argument('--predict_path', default='./data/raw/dev.csv')\n",
    "## predict_path에 dev.csv로 설정 >> dev set에 대한 predictions 출력\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# baseline_config 설정 불러오기\n",
    "with open('baselines/baseline_config.yaml', 'r', encoding='utf-8') as f:\n",
    "        CFG = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# inference에 쓸 모델 불러오기(CFG로 참조)\n",
    "model_path = CFG['inference']['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data_pipeline.Dataloader(CFG, args.train_path, args.dev_path, args.test_path, args.predict_path)\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (plm): ElectraForSequenceClassification(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): ElectraClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): GELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_func): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer로 쓸 ElectraTokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('snunlp/KR-ELECTRA-discriminator', max_length=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraTokenizerFast(name_or_path='snunlp/KR-ELECTRA-discriminator', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['sentence_1', 'sentence_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(dataframe):\n",
    "    data = []\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in text_columns])\n",
    "        # padding=True와 truncation=True 옵션을 명시적으로 추가합니다.\n",
    "        outputs = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',  # max_length로 패딩을 고정\n",
    "            truncation=True,       # 텍스트를 최대 길이로 자름\n",
    "            max_length=160         # max_length 설정\n",
    "        )\n",
    "\n",
    "        token_ids = outputs['input_ids']\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "\n",
    "        data.append(tokens)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/raw/train.csv')\n",
    "\n",
    "## spacing 유무에 따른 token 비교를 위해 원본 남겨두기\n",
    "df_origin = pd.read_csv('./data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacing에 쓸 kiwi\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def correct_spacing(text):\n",
    "    return kiwi.space(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame에 apply를 사용해 띄어쓰기 교정 적용\n",
    "df['sentence_1'] = df['sentence_1'].apply(correct_spacing)\n",
    "df['sentence_2'] = df['sentence_2'].apply(correct_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>boostcamp-sts-v1-train-5123</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>오프라인 오픈형 성인용품점을 업종 관리해 주세요.</td>\n",
       "      <td>오프라인 오픈형 성인 용품 매장을 관리해 주세요.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>boostcamp-sts-v1-train-1768</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>유치원, 어린이집 영어 수업 금지법 철회해 주시고 유치원, 어린이집 영어 강사들을 ...</td>\n",
       "      <td>유치원과 어린이집에서 영어 수업을 금지하는 법을 철회하고 유치원과 어린이집에서 영어...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>boostcamp-sts-v1-train-5608</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>반전 영화인 걸 알면서 봤는데 이렇게 뒷 통수를 칠 줄이야 ㅎㅎ</td>\n",
       "      <td>중학교 때 반전이란 걸 알게해 준 영화</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>boostcamp-sts-v1-train-353</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>부끄럽다고 사진에는 안 나왔습니다.</td>\n",
       "      <td>메일로는 안 왔어요 ~</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>boostcamp-sts-v1-train-7278</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>평점이 너무 높다 이 정돈 아님</td>\n",
       "      <td>평점이 너무 낮은 듯;;;</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id         source  \\\n",
       "5123  boostcamp-sts-v1-train-5123   petition-rtt   \n",
       "1768  boostcamp-sts-v1-train-1768   petition-rtt   \n",
       "5608  boostcamp-sts-v1-train-5608   nsmc-sampled   \n",
       "353    boostcamp-sts-v1-train-353  slack-sampled   \n",
       "7278  boostcamp-sts-v1-train-7278   nsmc-sampled   \n",
       "\n",
       "                                             sentence_1  \\\n",
       "5123                        오프라인 오픈형 성인용품점을 업종 관리해 주세요.   \n",
       "1768  유치원, 어린이집 영어 수업 금지법 철회해 주시고 유치원, 어린이집 영어 강사들을 ...   \n",
       "5608                반전 영화인 걸 알면서 봤는데 이렇게 뒷 통수를 칠 줄이야 ㅎㅎ   \n",
       "353                                 부끄럽다고 사진에는 안 나왔습니다.   \n",
       "7278                                  평점이 너무 높다 이 정돈 아님   \n",
       "\n",
       "                                             sentence_2  label  binary-label  \n",
       "5123                        오프라인 오픈형 성인 용품 매장을 관리해 주세요.    4.2           1.0  \n",
       "1768  유치원과 어린이집에서 영어 수업을 금지하는 법을 철회하고 유치원과 어린이집에서 영어...    4.4           1.0  \n",
       "5608                              중학교 때 반전이란 걸 알게해 준 영화    0.8           0.0  \n",
       "353                                        메일로는 안 왔어요 ~    0.0           0.0  \n",
       "7278                                     평점이 너무 낮은 듯;;;    0.8           0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=5, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "된거 같은데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>boostcamp-sts-v1-train-5123</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>오프라인 오픈형 성인용품점을 업종관리 해주세요.</td>\n",
       "      <td>오프라인 오픈형 성인용품 매장을 관리해주세요.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>boostcamp-sts-v1-train-1768</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>유치원, 어린이집 영어수업 금지법 철회 해주시고 유치원, 어린이집 영어강사들을 보호...</td>\n",
       "      <td>유치원과 어린이집에서 영어 수업을 금지하는 법을 철회하고 유치원과 어린이집에서 영어...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>boostcamp-sts-v1-train-5608</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>반전영화인걸 알면서 봤는데 이렇게 뒷통수를 칠줄이야 ㅎㅎ</td>\n",
       "      <td>중학교때 반전이란걸 알게해준 영화</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>boostcamp-sts-v1-train-353</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>부끄럽다고 사진에는 안나왔습니다.</td>\n",
       "      <td>메일로는 안왔어요 ~</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>boostcamp-sts-v1-train-7278</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>평점이 너무 높다 이정돈아님</td>\n",
       "      <td>평점이 너무 낮은 듯;;;</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id         source  \\\n",
       "5123  boostcamp-sts-v1-train-5123   petition-rtt   \n",
       "1768  boostcamp-sts-v1-train-1768   petition-rtt   \n",
       "5608  boostcamp-sts-v1-train-5608   nsmc-sampled   \n",
       "353    boostcamp-sts-v1-train-353  slack-sampled   \n",
       "7278  boostcamp-sts-v1-train-7278   nsmc-sampled   \n",
       "\n",
       "                                             sentence_1  \\\n",
       "5123                         오프라인 오픈형 성인용품점을 업종관리 해주세요.   \n",
       "1768  유치원, 어린이집 영어수업 금지법 철회 해주시고 유치원, 어린이집 영어강사들을 보호...   \n",
       "5608                    반전영화인걸 알면서 봤는데 이렇게 뒷통수를 칠줄이야 ㅎㅎ   \n",
       "353                                  부끄럽다고 사진에는 안나왔습니다.   \n",
       "7278                                    평점이 너무 높다 이정돈아님   \n",
       "\n",
       "                                             sentence_2  label  binary-label  \n",
       "5123                          오프라인 오픈형 성인용품 매장을 관리해주세요.    4.2           1.0  \n",
       "1768  유치원과 어린이집에서 영어 수업을 금지하는 법을 철회하고 유치원과 어린이집에서 영어...    4.4           1.0  \n",
       "5608                                 중학교때 반전이란걸 알게해준 영화    0.8           0.0  \n",
       "353                                         메일로는 안왔어요 ~    0.0           0.0  \n",
       "7278                                     평점이 너무 낮은 듯;;;    0.8           0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_origin.sample(n=5, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본과 비교하면 spacing이 적절하게 처리된것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 9324/9324 [00:01<00:00, 6558.52it/s]\n",
      "tokenizing: 100%|██████████| 9324/9324 [00:01<00:00, 7902.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_spaced = tokenizing(dataframe=df)\n",
    "tokenized_origin = tokenizing(dataframe=df_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '앗', '제', '##가', '접근', '##권', '##한', '##이', '없', '##다고', '뜹', '##니다', ';', ';', '[SEP]', '오', ',', '액', '##세스', '권한', '##이', '없', '##다고', '합니다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', '앗', '제', '##가', '접근', '권한', '##이', '없', '##다고', '뜹', '##니다', ';', ';', '[SEP]', '오', ',', '액', '##세스', '권한', '##이', '없', '##다고', '합니다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_origin[1])\n",
    "print(tokenized_spaced[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacing에 따라 토큰이 달라지는 것을 확인\n",
    "\n",
    "이 차이가 임베딩 이후에 어떤 결과 차이로 이어지는지까지 확인하고 싶었는데 실력 이슈로 보류 </br>\n",
    "다음 프로젝트에서는 임베딩 벡터의 변화 관찰, attention map 출력까지 해보고 싶다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
