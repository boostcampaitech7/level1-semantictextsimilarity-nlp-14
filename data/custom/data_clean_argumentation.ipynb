{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "df_tr = pd.read_csv(\"./data/raw/train.csv\")\n",
    "\n",
    "\n",
    "df_v = pd.read_csv(\"./data/raw/dev.csv\")\n",
    "\n",
    "\n",
    "df_tt = pd.read_csv(\"./data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>boostcamp-sts-v1-train-6801</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>영원히이루어질수없게되버린 핀첼커플</td>\n",
       "      <td>여인은 오늘도 쳇바퀴속에서 벗어날수 없는 현실로부터의 도피를 꿈꾼다</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>boostcamp-sts-v1-train-4108</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>진짜 죽기전에 봐야할 걸작선~!</td>\n",
       "      <td>죽기 전에 꼭 봐야할 영화 선</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>boostcamp-sts-v1-train-6722</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>오.. 단순 회의 참가는 VR이 없어도 되네요.</td>\n",
       "      <td>아.. VR이 필요없는 간단한 회의참가.</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>boostcamp-sts-v1-train-3649</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>비행기 타기가 귀찮아서 여행 안가는 저에겐 신기할 따름.</td>\n",
       "      <td>비행이 귀찮아서 여행을 하지 않는 저에게는 기적입니다.</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>boostcamp-sts-v1-train-9316</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>어쩌다 보니 사람 들어간 사진은 못찍었는데 너무 맛있었고 재밌었습니다!!</td>\n",
       "      <td>너무 맛있어서 또 가고싶은집!!</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id         source  \\\n",
       "6801  boostcamp-sts-v1-train-6801   nsmc-sampled   \n",
       "4108  boostcamp-sts-v1-train-4108   nsmc-sampled   \n",
       "6722  boostcamp-sts-v1-train-6722      slack-rtt   \n",
       "3649  boostcamp-sts-v1-train-3649      slack-rtt   \n",
       "9316  boostcamp-sts-v1-train-9316  slack-sampled   \n",
       "\n",
       "                                    sentence_1  \\\n",
       "6801                        영원히이루어질수없게되버린 핀첼커플   \n",
       "4108                         진짜 죽기전에 봐야할 걸작선~!   \n",
       "6722                오.. 단순 회의 참가는 VR이 없어도 되네요.   \n",
       "3649           비행기 타기가 귀찮아서 여행 안가는 저에겐 신기할 따름.   \n",
       "9316  어쩌다 보니 사람 들어간 사진은 못찍었는데 너무 맛있었고 재밌었습니다!!   \n",
       "\n",
       "                                 sentence_2  label  binary-label  \n",
       "6801  여인은 오늘도 쳇바퀴속에서 벗어날수 없는 현실로부터의 도피를 꿈꾼다    0.0           0.0  \n",
       "4108                       죽기 전에 꼭 봐야할 영화 선    3.8           1.0  \n",
       "6722                 아.. VR이 필요없는 간단한 회의참가.    3.8           1.0  \n",
       "3649         비행이 귀찮아서 여행을 하지 않는 저에게는 기적입니다.    3.2           1.0  \n",
       "9316                      너무 맛있어서 또 가고싶은집!!    1.6           0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pykospacing import Spacing\n",
    "\n",
    "spacing = Spacing()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # 쓸모없는 이모티콘(ㅋ, ㅎ, ㅇ, ㄷ 등) 제거\n",
    "    text = re.sub(r\"[ㅋㅎㄷㅇ]+\", \"\", text)\n",
    "\n",
    "    # 특수문자 제거 (문장부호는 남김)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9.,!? ]+\", \"\", text)\n",
    "\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 문장 속 .가 3개 이상인 경우 ...로 변경\n",
    "    text = re.sub(r\"\\.{4,}\", \"...\", text)\n",
    "\n",
    "    # 문장 속 ,가 3개 이상인 경우 ,,,로 변경\n",
    "    text = re.sub(r\"\\,{4,}\", \",,,\", text)\n",
    "\n",
    "    # 문장 속 !가 3개 이상인 경우 !로 변경\n",
    "    text = re.sub(r\"!{4,}\", \"!\", text)\n",
    "\n",
    "    # 문장 속 ?가 3개 이상인 경우 ???로 변경\n",
    "    text = re.sub(r\"\\?{4,}\", \"?\", text)\n",
    "\n",
    "    # 문장 속 ㅜ가 3개 이상인 경우 ㅠㅠ로 변경\n",
    "    text = re.sub(r\"ㅜ{1,}\", \"ㅠ\", text)\n",
    "\n",
    "    # 문장 속 ㅠ가 3개 이상인 경우 ㅠㅠ로 변경\n",
    "    text = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def processing(df):\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(spacing)\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(spacing)\n",
    "\n",
    "    # 문장 변경\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(clean_text)\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(clean_text)\n",
    "\n",
    "    if \"sentence_1_clean\" in df.columns:\n",
    "        df.drop(columns=[\"sentence_1_clean\"], inplace=True)\n",
    "\n",
    "    if \"sentence_2_clean\" in df.columns:\n",
    "        df.drop(columns=[\"sentence_2_clean\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = processing(df_tr)\n",
    "df_tr.to_csv(\"./data/custom/train_v1.0.5_clean_spacing.csv\", index=False)\n",
    "\n",
    "\n",
    "df_v = processing(df_v)\n",
    "df_v.to_csv(\"./data/custom/dev_v1.0.5_clean_spacing.csv\", index=False)\n",
    "\n",
    "\n",
    "df_tt = processing(df_tt)\n",
    "df_tt.to_csv(\"./data/custom/test_v1.0.5_clean_spacing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_types(df):\n",
    "    # 각 컬럼의 데이터 타입 확인\n",
    "    print(df[[\"sentence_1\", \"sentence_2\"]].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_1    object\n",
      "sentence_2    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "check_data_types(df_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문맥을 고려한 텍스트 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BERT_Augmentation:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"monologg/koelectra-base-v3-generator\"\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = transformers.pipeline(\n",
    "            \"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0\n",
    "        )\n",
    "        random.seed(42)\n",
    "\n",
    "    # 랜덤하게 토큰? 글자에 마스크 씌우기\n",
    "    def random_masking_replacement(self, sentence, ratio=0.3):\n",
    "        \"\"\"Masking random eojeol of the sentence, and recover them using PLM.\n",
    "\n",
    "        Args:|\n",
    "            sentence (str): Source sentence\n",
    "            ratio (int): Ratio of masking\n",
    "\n",
    "        Returns:\n",
    "          str: Recovered sentence\n",
    "        \"\"\"\n",
    "\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "\n",
    "        # 품질 유지를 위해, 문장의 어절 수가 4 이하라면 원문장을 그대로 리턴합니다.\n",
    "        if len(sentence.split()) <= 4:\n",
    "            return sentence\n",
    "\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        unmask_sentence = sentence\n",
    "        # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "        random_idx = random.randint(1, len(unmask_sentence.split()) - span)\n",
    "\n",
    "        unmask_sentence = unmask_sentence.split()\n",
    "        # del unmask_sentence[random_idx:random_idx+span]\n",
    "        cache = []\n",
    "        for _ in range(span):\n",
    "            # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "            while cache and random_idx in cache:\n",
    "                random_idx = random.randint(1, len(unmask_sentence) - 2)\n",
    "            cache.append(random_idx)\n",
    "            unmask_sentence[random_idx] = mask\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0][\"sequence\"]\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "        unmask_sentence = \" \".join(unmask_sentence)\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()\n",
    "\n",
    "    def random_masking_insertion(self, sentence, ratio=0.3):\n",
    "\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        # Recover\n",
    "        unmask_sentence = sentence\n",
    "\n",
    "        for _ in range(span):\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "            random_idx = random.randint(0, len(unmask_sentence) - 1)\n",
    "            unmask_sentence.insert(random_idx, mask)\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0][\"sequence\"]\n",
    "\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import random\n",
    "import requests\n",
    "from kiwipiepy import Kiwi\n",
    "import time\n",
    "from quickspacer import Spacer\n",
    "\n",
    "\n",
    "class AdverbAugmentation:\n",
    "    def __init__(self):\n",
    "        self.kiwi = Kiwi()\n",
    "        self.spacing = Spacer().space\n",
    "        self.gloss_cache = {}  # 캐시 추가\n",
    "\n",
    "    def _adverb_detector(self, sentence):\n",
    "\n",
    "        # POS info\n",
    "        pos_list = [(x[0], x[1]) for x in self.kiwi.tokenize(sentence)]  # (token, pos)\n",
    "\n",
    "        adverb_list = []\n",
    "        for pos in pos_list:\n",
    "            if pos[1] == \"MAG\" and len(pos[0]) > 1:  # 1음절 부사는 제외함.\n",
    "                adverb_list.append(pos[0])\n",
    "        return adverb_list\n",
    "\n",
    "    def _get_gloss(self, word):\n",
    "        if word in self.gloss_cache:  # 캐시 확인\n",
    "            return self.gloss_cache[word]\n",
    "\n",
    "        res = requests.get(\"https://dic.daum.net/search.do?q=\" + word, timeout=5)\n",
    "        time.sleep(random.uniform(0.5, 2.5))\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        try:\n",
    "            # 첫 번째 뜻풀이.\n",
    "            meaning = soup.find(\"span\", class_=\"txt_search\")\n",
    "        except AttributeError:\n",
    "            return word\n",
    "        if not meaning:\n",
    "            return word\n",
    "\n",
    "        # parsing 결과에서 한글만 추출\n",
    "        meaning = re.findall(\"[가-힣]+\", str(meaning))\n",
    "        meaning = \" \".join(meaning)\n",
    "\n",
    "        # 띄어쓰기 오류 교정 (위 에 -> 위에)\n",
    "        # meaning = spell_checker.check(meaning).as_dict()['checked'].strip()\n",
    "        meaning = self.spacing([meaning.replace(\" \", \"\")])\n",
    "\n",
    "        self.gloss_cache[word] = meaning[0].strip()  # 캐시에 저장\n",
    "\n",
    "        return meaning[0].strip()\n",
    "\n",
    "    def adverb_gloss_replacement(self, sentence):\n",
    "        adverb_list = self._adverb_detector(sentence)\n",
    "        if adverb_list:\n",
    "            # 부사들 중에서 1개만 랜덤으로 선택합니다.\n",
    "            adverb = random.choice(adverb_list)\n",
    "            try:\n",
    "                gloss = self._get_gloss(adverb)\n",
    "                sentence = sentence.replace(adverb, gloss)\n",
    "            except:\n",
    "                print(\"except: \", sentence)\n",
    "                pass\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "wordnet = {}\n",
    "with open(\"./data/custom/wordnet.pickle\", \"rb\") as f:\n",
    "    wordnet = pickle.load(f)\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    if len(new_words) != 0:\n",
    "        sentence = \" \".join(new_words)\n",
    "        new_words = sentence.split(\" \")\n",
    "\n",
    "    else:\n",
    "        new_words = \"\"\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synomyms = []\n",
    "\n",
    "    try:\n",
    "        for syn in wordnet[word]:\n",
    "            for s in syn:\n",
    "                synomyms.append(s)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return synomyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도 있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요</td>\n",
       "      <td>반전도 있고, 사랑도 있고 재미도 있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근 권한이 없다고 뜹니다</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약 조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사 후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다 가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boostcamp-sts-v1-train-005</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>오마이가 뜨지져 스크롸이 스트휏</td>\n",
       "      <td>오 마이 갓 지저스 스크론 이스트 팬</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>boostcamp-sts-v1-train-9319</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>교원능력개발평가에서 교원이 보호받을 수 있는 장치를 마련해야 합니다</td>\n",
       "      <td>본인이 납부한 국민연금 금액을 기준으로 대출을 받을 수 있는 제도를 마련해 주세요</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9320</th>\n",
       "      <td>boostcamp-sts-v1-train-9320</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>여성가족부의 폐지를 원합 니드</td>\n",
       "      <td>여성가족부 폐지를 청원 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>boostcamp-sts-v1-train-9321</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>국회의원들 월급 좀 줄여주세요</td>\n",
       "      <td>공무원 봉급 좀 줄이지 좀 마세요</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>boostcamp-sts-v1-train-9322</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>오늘 못한 점심은 다음에 다시 츄라이 하기로 해요!!</td>\n",
       "      <td>오늘 못 먹은 밥은 꼭 담에 먹기로 하고요!!</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>boostcamp-sts-v1-train-9323</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>법정 공휴일 휴무 관련 근로자</td>\n",
       "      <td>법정 공휴일의 유급휴무화를 막아야 합니다.</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7205 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id            source  \\\n",
       "0      boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1      boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2      boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3      boostcamp-sts-v1-train-003     slack-sampled   \n",
       "5      boostcamp-sts-v1-train-005          nsmc-rtt   \n",
       "...                           ...               ...   \n",
       "9319  boostcamp-sts-v1-train-9319  petition-sampled   \n",
       "9320  boostcamp-sts-v1-train-9320  petition-sampled   \n",
       "9321  boostcamp-sts-v1-train-9321  petition-sampled   \n",
       "9322  boostcamp-sts-v1-train-9322     slack-sampled   \n",
       "9323  boostcamp-sts-v1-train-9323  petition-sampled   \n",
       "\n",
       "                                  sentence_1  \\\n",
       "0     스릴도 있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요   \n",
       "1                        앗 제가 접근 권한이 없다고 뜹니다   \n",
       "2                            주택청약 조건 변경해주세요.   \n",
       "3                    입사 후 처음 대면으로 만나 반가웠습니다.   \n",
       "5                          오마이가 뜨지져 스크롸이 스트휏   \n",
       "...                                      ...   \n",
       "9319   교원능력개발평가에서 교원이 보호받을 수 있는 장치를 마련해야 합니다   \n",
       "9320                        여성가족부의 폐지를 원합 니드   \n",
       "9321                        국회의원들 월급 좀 줄여주세요   \n",
       "9322           오늘 못한 점심은 다음에 다시 츄라이 하기로 해요!!   \n",
       "9323                        법정 공휴일 휴무 관련 근로자   \n",
       "\n",
       "                                         sentence_2  label  binary-label  \n",
       "0                           반전도 있고, 사랑도 있고 재미도 있네요.    2.2           0.0  \n",
       "1                               오, 액세스 권한이 없다고 합니다.    4.2           1.0  \n",
       "2                                주택청약 무주택기준 변경해주세요.    2.4           0.0  \n",
       "3                     화상으로만 보다 가 리얼로 만나니 정말 반가웠습니다.    3.0           1.0  \n",
       "5                              오 마이 갓 지저스 스크론 이스트 팬    2.6           1.0  \n",
       "...                                             ...    ...           ...  \n",
       "9319  본인이 납부한 국민연금 금액을 기준으로 대출을 받을 수 있는 제도를 마련해 주세요    0.2           0.0  \n",
       "9320                              여성가족부 폐지를 청원 합니다.    4.2           1.0  \n",
       "9321                             공무원 봉급 좀 줄이지 좀 마세요    0.6           0.0  \n",
       "9322                      오늘 못 먹은 밥은 꼭 담에 먹기로 하고요!!    3.2           1.0  \n",
       "9323                        법정 공휴일의 유급휴무화를 막아야 합니다.    1.4           0.0  \n",
       "\n",
       "[7205 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr[df_tr[\"label\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yohan\\.virtualenvs\\level1-semantictextsimilarity-nlp-14-Qc70mCay\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Masking Insertion 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9324/9324 [03:02<00:00, 51.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부사 치환 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9324/9324 [14:07<00:00, 11.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "df = df_tr\n",
    "\n",
    "original_data = df.copy()\n",
    "\n",
    "# BERT 증강 및 부사 치환 증강 클래스 인스턴스 생성\n",
    "BERT_aug = BERT_Augmentation()\n",
    "random_masking_replacement = BERT_aug.random_masking_replacement\n",
    "random_masking_insertion = BERT_aug.random_masking_insertion\n",
    "adverb_aug = AdverbAugmentation()\n",
    "adverb_gloss_replacement = adverb_aug.adverb_gloss_replacement\n",
    "\n",
    "\n",
    "# 증강 함수 정의\n",
    "def augment_sentence(row, aug_method):\n",
    "    row[\"sentence_1\"] = aug_method(row[\"sentence_1\"])\n",
    "    row[\"sentence_2\"] = aug_method(row[\"sentence_2\"])\n",
    "    return row\n",
    "\n",
    "\n",
    "# 각 증강 기법에 따라 데이터셋 증강\n",
    "def augment_dataset(df, aug_method, source_label, n_jobs=1, num_aug=1):\n",
    "    augmented_rows = []\n",
    "    for _ in range(num_aug):  # 각 문장에 대해 num_aug 만큼 증강\n",
    "        augmented_rows += Parallel(n_jobs=n_jobs)(\n",
    "            delayed(augment_sentence)(row.copy(), aug_method)\n",
    "            for idx, row in tqdm(df.iterrows(), total=len(df))\n",
    "        )\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df[\"source\"] = source_label  # 증강 방법에 따라 source 설정\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "# 증강 기법 적용\n",
    "\n",
    "# 1. Random Masking Insertion (BERT)\n",
    "print(\"Random Masking Insertion 중...\")\n",
    "masking_insertion_augset = augment_dataset(\n",
    "    df.copy(), lambda x: random_masking_insertion(x, ratio=0.15), \"data-argu-RMI\"\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 부사 치환\n",
    "print(\"부사 치환 중...\")\n",
    "adverb_augset = augment_dataset(df.copy(), adverb_gloss_replacement, \"data-argu-adverb\")\n",
    "\n",
    "\n",
    "# 원본 데이터와 결합 및 중복 제거\n",
    "augmented_data = pd.concat([masking_insertion_augset, adverb_augset])\n",
    "augmented_data = augmented_data.drop_duplicates(\n",
    "    subset=[\"sentence_1\", \"sentence_2\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 증강 데이터에 id 업데이트\n",
    "augmented_data[\"id\"] = [\"argu-{:03d}\".format(i) for i in range(len(augmented_data))]\n",
    "\n",
    "# 원본 데이터와 결합 및 중복 제거\n",
    "final_data = (\n",
    "    pd.concat([original_data, augmented_data])\n",
    "    .drop_duplicates(subset=[\"sentence_1\", \"sentence_2\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "final_data.to_csv(\"./data/custom/train_v1.4.5_cl_argu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data[final_data[\"source\"] != \"data-argu-adverb\"]\n",
    "\n",
    "final_data.to_csv(\"./data/custom/train_v1.4.5_cl_argu_noadverb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
