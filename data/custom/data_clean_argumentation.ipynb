{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tr = pd.read_csv(\"./train_v1.0.2_clean_spacing.csv\")\n",
    "df_v = pd.read_csv(\"../raw/dev.csv\")\n",
    "df_tt = pd.read_csv(\"../raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>boostcamp-sts-v1-train-1808</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>돈 내고 보기 아까운 영화 네요..</td>\n",
       "      <td>오래 만에 감동을 불러 일으키는 영화 네요..</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>boostcamp-sts-v1-train-3200</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>미세먼지 많이 마시고 수술 받은 사람입니다</td>\n",
       "      <td>추천 누르시고 이명박이랑 미세먼지 동의 해주세요</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>boostcamp-sts-v1-train-5260</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>그분은 항상 제 편을 들어주셨었습니다.</td>\n",
       "      <td>아버지도 제 편이 아니었고 어머니도 같이 있지 않을 때 끝까지 제 편이었던 선생님....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>boostcamp-sts-v1-train-123</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>집값 안정화에 대한 청원</td>\n",
       "      <td>집값 안정화를 위한 제안</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boostcamp-sts-v1-train-007</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>이렇게 귀여운 쥐들은 처음이네요. ㅎㅎㅎ</td>\n",
       "      <td>이렇게 지 겨운 공포영화는 처음..</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id            source               sentence_1  \\\n",
       "1808  boostcamp-sts-v1-train-1808      nsmc-sampled      돈 내고 보기 아까운 영화 네요..   \n",
       "3200  boostcamp-sts-v1-train-3200  petition-sampled  미세먼지 많이 마시고 수술 받은 사람입니다   \n",
       "5260  boostcamp-sts-v1-train-5260      nsmc-sampled    그분은 항상 제 편을 들어주셨었습니다.   \n",
       "123    boostcamp-sts-v1-train-123  petition-sampled            집값 안정화에 대한 청원   \n",
       "7      boostcamp-sts-v1-train-007      nsmc-sampled   이렇게 귀여운 쥐들은 처음이네요. ㅎㅎㅎ   \n",
       "\n",
       "                                             sentence_2  label  binary-label  \n",
       "1808                          오래 만에 감동을 불러 일으키는 영화 네요..    0.6           0.0  \n",
       "3200                         추천 누르시고 이명박이랑 미세먼지 동의 해주세요    0.6           0.0  \n",
       "5260  아버지도 제 편이 아니었고 어머니도 같이 있지 않을 때 끝까지 제 편이었던 선생님....    1.0           0.0  \n",
       "123                                       집값 안정화를 위한 제안    3.2           1.0  \n",
       "7                                   이렇게 지 겨운 공포영화는 처음..    0.6           0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pykospacing import Spacing\n",
    "\n",
    "spacing = Spacing()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # 문장 속 / 삭제\n",
    "    text = re.sub(r\"/\", \" \", text)\n",
    "\n",
    "    # 문장 속 ㅋ가 4개 이상 일 경우 ㅋㅋㅋ으로 변경\n",
    "    text = re.sub(r\"ㅋ{4,}\", \"ㅋㅋㅋ\", text)\n",
    "\n",
    "    # 문장 속 ㅎ가 3개 이상인 경우 ㅎ로 변경\n",
    "    text = re.sub(r\"ㅎ{4,}\", \"ㅎㅎㅎ\", text)\n",
    "\n",
    "    # 문장 속 ~가 1개 이상인 경우 ~로 변경\n",
    "    text = re.sub(r\"~{2,}\", \"~\", text)\n",
    "\n",
    "    # 문장 속 ;가 3개 이상인 경우 ;;로 변경\n",
    "    text = re.sub(r\";{3,}\", \";;\", text)\n",
    "\n",
    "    # 문장 속 .가 3개 이상인 경우 ...로 변경\n",
    "    text = re.sub(r\"\\.{4,}\", \"...\", text)\n",
    "\n",
    "    # 문장 속 ,가 3개 이상인 경우 ,,,로 변경\n",
    "    text = re.sub(r\"\\,{4,}\", \",,,\", text)\n",
    "\n",
    "    # 문장 속 !가 3개 이상인 경우 !!!로 변경\n",
    "    text = re.sub(r\"!{4,}\", \"!!!\", text)\n",
    "\n",
    "    # 문장 속 ?가 3개 이상인 경우 ???로 변경\n",
    "    text = re.sub(r\"\\?{4,}\", \"???\", text)\n",
    "\n",
    "    # 문장 속 ^가 3개 이상인 경우 ^^로 변경\n",
    "    text = re.sub(r\"\\^{3,}\", \"^^\", text)\n",
    "\n",
    "    # 문장 속 ㅠ가 3개 이상인 경우 ㅠㅠ로 변경\n",
    "    text = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", text)\n",
    "\n",
    "    # 문장 속 ㅜ가 3개 이상인 경우 ㅜㅜ로 변경\n",
    "    text = re.sub(r\"ㅜ{3,}\", \"ㅠㅠ\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def processing(df):\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(spacing)\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(spacing)\n",
    "\n",
    "    # 문장 변경\n",
    "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(clean_text)\n",
    "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(clean_text)\n",
    "\n",
    "    if \"sentence_1_clean\" in df.columns:\n",
    "        df.drop(columns=[\"sentence_1_clean\"], inplace=True)\n",
    "\n",
    "    if \"sentence_2_clean\" in df.columns:\n",
    "        df.drop(columns=[\"sentence_2_clean\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = processing(df_tr)\n",
    "df_v = processing(df_v)\n",
    "df_tt = processing(df_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문맥을 고려한 텍스트 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BERT_Augmentation:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"monologg/koelectra-base-v3-generator\"\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = transformers.pipeline(\n",
    "            \"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0\n",
    "        )\n",
    "        random.seed(42)\n",
    "\n",
    "    # 랜덤하게 토큰? 글자에 마스크 씌우기\n",
    "    def random_masking_replacement(self, sentence, ratio=0.15):\n",
    "        \"\"\"Masking random eojeol of the sentence, and recover them using PLM.\n",
    "\n",
    "        Args:|\n",
    "            sentence (str): Source sentence\n",
    "            ratio (int): Ratio of masking\n",
    "\n",
    "        Returns:\n",
    "          str: Recovered sentence\n",
    "        \"\"\"\n",
    "\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "\n",
    "        # 품질 유지를 위해, 문장의 어절 수가 4 이하라면 원문장을 그대로 리턴합니다.\n",
    "        if len(sentence.split()) <= 4:\n",
    "            return sentence\n",
    "\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        unmask_sentence = sentence\n",
    "        # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "        random_idx = random.randint(1, len(unmask_sentence.split()) - span)\n",
    "\n",
    "        unmask_sentence = unmask_sentence.split()\n",
    "        # del unmask_sentence[random_idx:random_idx+span]\n",
    "        cache = []\n",
    "        for _ in range(span):\n",
    "            # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음.\n",
    "            while cache and random_idx in cache:\n",
    "                random_idx = random.randint(1, len(unmask_sentence) - 2)\n",
    "            cache.append(random_idx)\n",
    "            unmask_sentence[random_idx] = mask\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0][\"sequence\"]\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "        unmask_sentence = \" \".join(unmask_sentence)\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()\n",
    "\n",
    "    def random_masking_insertion(self, sentence, ratio=0.15):\n",
    "\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        # Recover\n",
    "        unmask_sentence = sentence\n",
    "\n",
    "        for _ in range(span):\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "            random_idx = random.randint(0, len(unmask_sentence) - 1)\n",
    "            unmask_sentence.insert(random_idx, mask)\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0][\"sequence\"]\n",
    "\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import random\n",
    "import requests\n",
    "from kiwipiepy import Kiwi\n",
    "import time\n",
    "from quickspacer import Spacer\n",
    "\n",
    "\n",
    "class AdverbAugmentation:\n",
    "    def __init__(self):\n",
    "        self.kiwi = Kiwi()\n",
    "        self.spacing = Spacer().space\n",
    "        self.gloss_cache = {}  # 캐시 추가\n",
    "\n",
    "    def _adverb_detector(self, sentence):\n",
    "\n",
    "        # POS info\n",
    "        pos_list = [(x[0], x[1]) for x in self.kiwi.tokenize(sentence)]  # (token, pos)\n",
    "\n",
    "        adverb_list = []\n",
    "        for pos in pos_list:\n",
    "            if pos[1] == \"MAG\" and len(pos[0]) > 1:  # 1음절 부사는 제외함.\n",
    "                adverb_list.append(pos[0])\n",
    "        return adverb_list\n",
    "\n",
    "    def _get_gloss(self, word):\n",
    "        if word in self.gloss_cache:  # 캐시 확인\n",
    "            return self.gloss_cache[word]\n",
    "\n",
    "        res = requests.get(\"https://dic.daum.net/search.do?q=\" + word, timeout=5)\n",
    "        time.sleep(random.uniform(0.5, 2.5))\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        try:\n",
    "            # 첫 번째 뜻풀이.\n",
    "            meaning = soup.find(\"span\", class_=\"txt_search\")\n",
    "        except AttributeError:\n",
    "            return word\n",
    "        if not meaning:\n",
    "            return word\n",
    "\n",
    "        # parsing 결과에서 한글만 추출\n",
    "        meaning = re.findall(\"[가-힣]+\", str(meaning))\n",
    "        meaning = \" \".join(meaning)\n",
    "\n",
    "        # 띄어쓰기 오류 교정 (위 에 -> 위에)\n",
    "        # meaning = spell_checker.check(meaning).as_dict()['checked'].strip()\n",
    "        meaning = self.spacing([meaning.replace(\" \", \"\")])\n",
    "\n",
    "        self.gloss_cache[word] = meaning[0].strip()  # 캐시에 저장\n",
    "\n",
    "        return meaning[0].strip()\n",
    "\n",
    "    def adverb_gloss_replacement(self, sentence):\n",
    "        adverb_list = self._adverb_detector(sentence)\n",
    "        if adverb_list:\n",
    "            # 부사들 중에서 1개만 랜덤으로 선택합니다.\n",
    "            adverb = random.choice(adverb_list)\n",
    "            try:\n",
    "                gloss = self._get_gloss(adverb)\n",
    "                sentence = sentence.replace(adverb, gloss)\n",
    "            except:\n",
    "                print(\"except: \", sentence)\n",
    "                pass\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "wordnet = {}\n",
    "with open(\"./wordnet.pickle\", \"rb\") as f:\n",
    "    wordnet = pickle.load(f)\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    if len(new_words) != 0:\n",
    "        sentence = \" \".join(new_words)\n",
    "        new_words = sentence.split(\" \")\n",
    "\n",
    "    else:\n",
    "        new_words = \"\"\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synomyms = []\n",
    "\n",
    "    try:\n",
    "        for syn in wordnet[word]:\n",
    "            for s in syn:\n",
    "                synomyms.append(s)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return synomyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Masking Insertion 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9324 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 9324/9324 [03:06<00:00, 50.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부사 치환 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 542/9324 [03:59<28:46,  5.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  순진+어리버리 연기를 했던 이완.. 짱 귀여움\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 952/9324 [05:29<21:45,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  아래 구글 시트에 내일 17:00 전까지 꼭 투표 및 정보기재 부탁드립니다~!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2402/9324 [08:49<15:48,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일은 드디어 첫 상봉의 날입니다. 내일 봐요!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 2448/9324 [08:53<10:19, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제 때문에 청원합니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 2453/9324 [08:56<16:32,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제 하고 싶어도 못하네요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4180/9324 [11:09<08:29, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  그럼 내일 우리 팀은 무조건 이기는 겁니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4378/9324 [11:19<05:47, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제는 의 미 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4516/9324 [11:22<02:16, 35.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제 관련된 민원입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4655/9324 [11:31<04:29, 17.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일 여기서 일 등 하면 좋겠습니다.\n",
      "except:  내일채움 청년제도 개선을 바랍니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4662/9324 [11:34<07:30, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제 제도를 개선해 주세요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 5902/9324 [12:34<02:37, 21.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일 예약 가능한 걸로 보이네용 !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6437/9324 [13:11<02:26, 19.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일 출근하는데 제가 갈 때 맞춰서 폭설이 흑..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 6606/9324 [13:21<02:01, 22.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일배움공제 제도를 보완해 주십시요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 6657/9324 [13:23<01:55, 23.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년내일채움공제 2017년 12월 입사자\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 6838/9324 [13:31<01:08, 36.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일 산행 준비 중입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7239/9324 [13:47<01:40, 20.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  와~ 이것 때문에 저 내일 사무실 출근합니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7254/9324 [13:49<01:57, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  내일 무료체험은 오후 2시입니다~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 7442/9324 [13:57<01:42, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except:  청년 내일채움공제의 신청 자격을 확대해 주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9324/9324 [15:06<00:00, 10.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "df = df_tr\n",
    "\n",
    "original_data = df.copy()\n",
    "\n",
    "# BERT 증강 및 부사 치환 증강 클래스 인스턴스 생성\n",
    "BERT_aug = BERT_Augmentation()\n",
    "random_masking_replacement = BERT_aug.random_masking_replacement\n",
    "random_masking_insertion = BERT_aug.random_masking_insertion\n",
    "adverb_aug = AdverbAugmentation()\n",
    "adverb_gloss_replacement = adverb_aug.adverb_gloss_replacement\n",
    "\n",
    "\n",
    "# 증강 함수 정의\n",
    "def augment_sentence(row, aug_method):\n",
    "    row[\"sentence_1\"] = aug_method(row[\"sentence_1\"])\n",
    "    row[\"sentence_2\"] = aug_method(row[\"sentence_2\"])\n",
    "    return row\n",
    "\n",
    "\n",
    "# 각 증강 기법에 따라 데이터셋 증강\n",
    "def augment_dataset(df, aug_method, source_label, n_jobs=1, num_aug=1):\n",
    "    augmented_rows = []\n",
    "    for _ in range(num_aug):  # 각 문장에 대해 num_aug 만큼 증강\n",
    "        augmented_rows += Parallel(n_jobs=n_jobs)(\n",
    "            delayed(augment_sentence)(row.copy(), aug_method)\n",
    "            for idx, row in tqdm(df.iterrows(), total=len(df))\n",
    "        )\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df[\"source\"] = source_label  # 증강 방법에 따라 source 설정\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "# 원본 라벨 유지\n",
    "original_data[\"source\"] = \"original\"\n",
    "\n",
    "# 증강 기법 적용\n",
    "\n",
    "# 1. Random Masking Insertion (BERT)\n",
    "print(\"Random Masking Insertion 중...\")\n",
    "masking_insertion_augset = augment_dataset(\n",
    "    df.copy(), lambda x: random_masking_insertion(x, ratio=0.15), \"data-argu-RMI\"\n",
    ")\n",
    "\n",
    "# 2. 부사 치환\n",
    "print(\"부사 치환 중...\")\n",
    "adverb_augset = augment_dataset(df.copy(), adverb_gloss_replacement, \"data-argu-adverb\")\n",
    "\n",
    "# 원본 데이터와 결합 및 중복 제거\n",
    "augmented_data = pd.concat([masking_insertion_augset, adverb_augset])\n",
    "augmented_data = augmented_data.drop_duplicates(\n",
    "    subset=[\"sentence_1\", \"sentence_2\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 증강 데이터에 id 업데이트\n",
    "augmented_data[\"id\"] = [\"argu-{:03d}\".format(i) for i in range(len(augmented_data))]\n",
    "\n",
    "# 원본 데이터와 결합 및 중복 제거\n",
    "final_data = (\n",
    "    pd.concat([original_data, augmented_data])\n",
    "    .drop_duplicates(subset=[\"sentence_1\", \"sentence_2\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "final_data.to_csv(\"./test_argu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
